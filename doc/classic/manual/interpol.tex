% interpol.tex
\section{Time Stamp Correlation} \label{sec:timestampcorr}
We have stressed more than once how the Channel Archiver tries to preserve
the original time stamps as sent by the CA servers.  This commonly
leads to difficulties when comparing values from different
channels. The following subsections investigate the issue in more
detail and show several ways of manipulating the data in order to
allow data reduction and cross-channel comparisons.
In short, the options described in the following subsections are:
\begin{description}
\item[\sffamily Raw Data:]
  Provides every archived sample ``as is''.
\item[\sffamily Spreadsheet:]
  Staircase interpolation/filling to form a spreadsheet.
\item[\sffamily Averaging, Linear Interpolation:]
  Maps the raw data onto specific time stamps.
\item[\sffamily Plot Binning:]
  Reduces the number of samples for plotting.
\end{description}

\subsection{``Raw'' Data} \label{sec:rawdata}
Even when two channels were served by the same IOC, and originating from
records on the same scan rate, their time stamps will slightly differ
because a single CPU cannot scan several channels at exactly the same
time.  Tab.~\ref{tab:ABtimes} shows one example.

\begin{table}[htbp]
  \begin{center}
    \begin{minipage}[t]{0.49\textwidth}
      \sffamily
      \begin{tabular}[t]{l|l}
        Time               & A         \\
        \hline
        17:02:28.700986000 & 0.0718241 \\
        17:02:37.400964000 & 0.0543581 \\
        ...
      \end{tabular}
    \end{minipage}%
    \begin{minipage}[t]{0.49\textwidth}
      \sffamily
      \begin{tabular}[t]{l|l}
        Time               & B         \\
        \hline
        17:02:28.701046000 & -0.086006 \\
        17:02:37.510961000 & -0.111776 \\
        ...
      \end{tabular}
    \end{minipage}%
    \caption{Example Time Stamps for two Channels A and B.}
    \label{tab:ABtimes}
  \end{center}
\end{table}

\noindent When we try to export this data in what we call \INDEX{raw
spreadsheet format}, a problem arises: Even though the two channels'
time stamps are close, they do not match, resulting in a spreadsheet
as shown in Tab.~\ref{tab:ABraw}. Whenever one channel has a value,
the other channel has none and vice versa.  This spreadsheet does
not yield itself to further analysis; calculations like $A-B$ will
always yield \INDEX{'\#N/A'} since either A or B is undefined.

\begin{table}[htbp]
  \begin{center}
    \sffamily
    \begin{tabular}[t]{l|l|l}
      Time                         & A         & B         \\
      \hline
      3/22/2000 17:02:28.700986000 & 0.0718241 & ---       \\
      3/22/2000 17:02:28.701046000 & ---       & -0.086006 \\
      3/22/2000 17:02:37.400964000 & 0.0543581 & ---       \\
      3/22/2000 17:02:37.510961000 & ---       & -0.111776 \\
      ...
    \end{tabular}
    \caption{Spreadsheet for raw Channels A and B.}
    \label{tab:ABraw}
  \end{center}
\end{table}

\subsection{``Before or at'' Interpretation of Start Times}
 \label{sec:starttime}
When you invoke a retrieval tool with a certain start time, the
archive will rarely contain a sample for that exact start time. As an
example, you might ask for a start time of ``07:00:00'' on some date. Not
because you expect to find a sample with that exact time stamp, but
because you want to look at data from the beginning of that day's
operations shift, which nominally began at 7am.

The software underlying all retrieval tools anticipates this scenario
by interpreting all start times as ``before or at''. Given a start time
of ``07:00:00'', it returns the last sample before that start
time, unless an exact match is found. So in case a sample for the
exact start time exists, it will of course be returned. But if the
archive contains no such sample, the previous sample is returned.

Applied to Tab.~\ref{tab:ABraw}, channel A, you would get the samples
shown in there not only if you asked for ``17:02:28.700986000'', the
exact start time, but also if you asked for ``17:02:30''.
It is left to the end user to decide whether that previous sample is
still useful at the requested start time, if it's ``close enough'', or
if it needs to be ignored.

\subsection{Spreadsheet Generation} \label{sec:filling}
There are several ways of mapping channels onto matching time
stamps. One is what we call \INDEX{Staircase Interpolation} or
\INDEX{Filling}: Whenever there is no current value for a channel, we
re-use the previous value. This is often perfectly acceptable because
the CA server will only send updates whenever a channel changes beyond
the configured deadband. So if we monitored a channel and did not
receive a new value, this means that the previous value is still valid
--- at least within the configured deadband. In the case of scanned
channels we have no idea how a channel behaved in between scans, but
if we e.g.\ look at water temperatures, it might be safe to assume
that the previous value is still ``close enough''.
Table~\ref{tab:ABstair} shows the previously discussed data subjected
to staircase interpolation. Note that in this example there is no
initial value for channel B, resulting in one empty spreadsheet
cell. From then on, however, there are always values for both
channels, because any missing samples are filled by repeating the
previous one.  Because of the interpretation of start times explained
in section \ref{sec:starttime}, you would get the result in
 Tab.\ \ref{tab:ABstair} not only if you asked for values beginning
``3/22/2000 17:02:28.700986000'', but also when you asked for
e.g. ``17:02:30'': Since neither channel A nor B have a sample for that
exact time stamp, the retrieval library would select the
preceding sample for each channel, resulting in the output shown in 
Tab.\ \ref{tab:ABstair}.

\NOTE While table~\ref{tab:ABstair} marks the filled values by
printing them in italics, spreadsheets generated by archive retrieval
tools will not accent the filled values in any way, so care must be
taken: Those filled values carry artificial time stamps. If you depend
on the original time stamps in order to synchronize certain events,
you must not use any form of interpolation but always retrieve the raw
data. 

\begin{table}[htbp]
  \begin{center}
    \sffamily
    \begin{tabular}[t]{l|l|l}
      Time                         & A         & B         \\
      \hline
      3/22/2000 17:02:28.700986000 & 0.0718241 & ---       \\
      3/22/2000 17:02:28.701046000 & \textit{0.0718241}      & -0.086006 \\
      3/22/2000 17:02:37.400964000 & 0.0543581 & \textit{-0.086006} \\
      3/22/2000 17:02:37.510961000 & \textit{0.0543581} & -0.111776 \\
      ...
    \end{tabular}
    \caption{Spreadsheet for Channels A and B with Staircase
      Interpolation; ``filled'' values shown in italics.}
    \label{tab:ABstair}
  \end{center}
\end{table}

You did of course notice that the staircase interpolation does not
reduce the amount of data. Quite the opposite: In the above examples,
channels A and B each had 2 values. With staircase interpolation, we
don't get a spreadsheet with 2 lines of data but 4 lines of data.  The
main advantage of filling lies is the preservation of original time stamps.

\subsection{Averaging, Linear Interpolation}  \label{sec:lininterpol}
Both averaging and linear interpolation generate artificial values from the raw
data. This can be used to reduce the amount of data: For a summary of
the last day, it might be sufficient to look at one value every 30
minutes, even though the archive could contain much more data.
Another aspect is partly cosmetic and partly a matter of convenience:
When we look at Tab.~\ref{tab:ABstair}, we find rather odd looking
time stamps. While these reflect the real time stamps that the
ArchiveEngine received from the ChannelAccess server, it is often
preferable to deal with data that has time stamps which are nicely
aligned, for example every 10 seconds: 11:20:00, 11:20:10, 11:20:20,
11:20:30 and so on.

\begin{figure}[htb]
\begin{center}
\medskip
\InsertImage{width=\textwidth}{images/interpol}
\end{center}
\caption{\label{fig:interpol}Averaging and Linear Interpolation, see text.}
\end{figure}

\noindent To accomplish this, the data is binned. For example, the
time span of one day, 24~hours, can be divided into 2880 sections,
each of which covers 30 seconds. Each of those sections is called a
``Bin''. The raw samples for the day are then investigated as follows:
\begin{itemize}
\item When we select \INDEX{Averaging}, the average over all the
  samples that fall into a bin is returned. The center of the bin is
  used as a time stamp.
\item When we select \INDEX{Linear Interpolation}, the value of the
  channel at the border of each bin is determined via linear
  interpolation between the last sample before and the first sample
  after the border of the bin.
  The border of each bin determines the time stamp.
\end{itemize}

\noindent Fig.~\ref{fig:interpol} compares the result of retrieving
the raw data with averaging and linear interpolation over
10-second-bins.  Averages are determined for the center of each bin,
i.e. 08:48:35, 08:48:45, ..., while linear interpolation generates
values for the bin-borders at 08:48:40, 08:48:50, ...  The linearly
interpolated values do in fact exactly fall onto the connecting lines
between raw samples if you consider the full time stamps, but the
plotting program chosen to produce fig.~\ref{fig:interpol} rounds down
to full seconds.

Note the gap just before 08:49:30: Since the channel was disconnected,
no average is returned for the bin from 08:49:20 to 08:49:30. 
Averaging and linear interpolation are further limited to scalar, numeric
samples of type double, float or int. Arrays or strings will not be
interpolated.

\NOTE Averaging and linear interpolation must be used with caution.
Both methods can hide important details in the raw data, and it is up
to the user to determine when to use them and with what bin size.

More recent versions of the averaging code return not only the average
value but also the minumum and maximum value within a bin.
In addition, it returns only the original value in case there was only
a single value in a bin.
In combination, this offers good data reduction for plotting,
since you can display the variation (min...max) around the average,
and finally get the raw data when zooming in far enough (small bin sizes).

\subsection{Plot-Binning} \label{sec:plotbinning}
\begin{figure}[htb]
\begin{center}
\medskip
\InsertImage{width=\textwidth}{images/plotbin}
\end{center}
\caption{\label{fig:plotbin}Plot-Binning, see text.}
\end{figure}

\NOTE This method was meant for plotting, but has been superseded by the
most recent averaging mechanism.

\noindent It provides data that --- when
plotted --- looks very much if not exactly like the raw data, albeit
significantly reducing the number of data points and hence speeding up
the plot. To accomplish this, the data is binned as follows:
For each bin, the initial,  minimum, maximum, and final sample is
determined. In case the bin contains 'info' samples like 'disconnected'
or 'archive off', the last of them is also kept.
Those up to 5 samples are then sorted in time and duplicates are removed.
So if a bin contains 0, 1 or two samples, the result contains those original
samples. Otherwise, the result might contain up to 5 samples per bin for the
initial, minimum, maximum, last 'info' and final sample in time order.

\noindent Fig.~\ref{fig:interpol} compares the raw data of a Klystron
test run, 2400 samples, with the result of plot-binning, bin size 600
seconds, yielding around 280 samples. While plot binning significantly
reduced the sample count, the overall shape of the klystron output as
well as the outliers are well preserved.

Note that the result does not provide any hints as to whether you are looking
at the "initial" sample of a bin or the "final".
As long as we plot the resulting stream of samples such that the width of
the plot in pixels is close to the number of bins, there is little visual
difference between the raw data plot and the binned plot.  Typical
numbers for $N$ are around the width of a computer screen in pixels,
that is 800\ldots 1200.  
